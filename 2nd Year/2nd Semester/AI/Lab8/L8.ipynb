{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T16:17:42.969900Z",
     "start_time": "2025-04-28T16:17:42.957855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "id": "98fe26c7de565c38",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T16:17:43.722286Z",
     "start_time": "2025-04-28T16:17:43.700908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Load the data\n",
    "\n",
    "crtDir =  os.getcwd()\n",
    "fileName = os.path.join(crtDir, 'data', 'reviews_mixed.csv')\n",
    "\n",
    "data = []\n",
    "with open(fileName) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            dataNames = row\n",
    "        else:\n",
    "            data.append(row)\n",
    "        line_count += 1\n",
    "\n",
    "inputs = [data[i][0] for i in range(len(data))][:100]\n",
    "outputs = [data[i][1] for i in range(len(data))][:100]\n",
    "labelNames = list(set(outputs))\n",
    "\n",
    "print(inputs[:2])\n",
    "print(labelNames[:2])"
   ],
   "id": "475b83946a70fc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The rooms are extremely small, practically only a bed.', 'Room safe did not work.']\n",
      "['negative', 'positive']\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T16:22:30.002609Z",
     "start_time": "2025-04-28T16:22:29.981910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2: Split the data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(5)\n",
    "# noSamples = inputs.shape[0]\n",
    "noSamples = len(inputs)\n",
    "indexes = [i for i in range(noSamples)]\n",
    "trainSample = np.random.choice(indexes, int(0.8 * noSamples), replace = False)\n",
    "testSample = [i for i in indexes  if not i in trainSample]\n",
    "\n",
    "trainInputs = [inputs[i] for i in trainSample]\n",
    "trainOutputs = [outputs[i] for i in trainSample]\n",
    "testInputs = [inputs[i] for i in testSample]\n",
    "testOutputs = [outputs[i] for i in testSample]\n",
    "\n",
    "print(trainInputs[:3])\n",
    "print(testInputs[:3])"
   ],
   "id": "708c03677a88f356",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Just to give you an idea: the shutters of the windows were not working, did not go neither up or down - just hanging down only one side and the other up....', 'and hip and CLEAN!', \"Toilet paper wasn't replaced everyday!\"]\n",
      "['The bed is very comfortable.', 'Very spacious rooms, quiet and very comfortable.', 'Corridors filthy\\nRoom filthy\\nElectrical cables in room not safe\\nWhole building smelly\\nShower repulsive']\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T16:24:36.683385Z",
     "start_time": "2025-04-28T16:24:35.881777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "# Step 3: Feature extraction\n",
    "# Mod 1: TF_IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "\n",
    "# trainFeatures = vectorizer.fit_transform(trainInputs)\n",
    "# testFeatures = vectorizer.transform(testInputs)\n",
    "\n",
    "#Mod 2: Doc2Vec\n",
    "tagged_train_texts = [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(trainInputs)]\n",
    "tagged_test_texts = [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(testInputs)]\n",
    "\n",
    "#fiecare document: vector, size=100\n",
    "doc2vec_model = Doc2Vec(vector_size=100, min_count=2, epochs=100, workers=4)\n",
    "doc2vec_model.build_vocab(tagged_train_texts)\n",
    "doc2vec_model.train(tagged_train_texts, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "\n",
    "trainFeatures = np.array([doc2vec_model.infer_vector(text.split()) for text in trainInputs])\n",
    "testFeatures = np.array([doc2vec_model.infer_vector(text.split()) for text in testInputs])\n",
    "\n",
    "\n"
   ],
   "id": "e75fe21a4997492e",
   "outputs": [],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T16:17:48.747324Z",
     "start_time": "2025-04-28T16:17:48.732171Z"
    }
   },
   "cell_type": "code",
   "source": "from MyANN import MyANN\n",
   "id": "ee12cf298513ba41",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T16:24:38.637314Z",
     "start_time": "2025-04-28T16:24:38.567912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 4: Train a model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "supervisedClassifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "supervisedClassifier.fit(trainFeatures, trainOutputs)\n",
    "\n",
    "###########################################################\n",
    "\n",
    "#Train my model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "model = MyANN()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "trainOutputsModel = label_encoder.fit_transform(trainOutputs)\n",
    "testOutputsModel = label_encoder.transform(testOutputs)\n",
    "\n",
    "model.fit(trainFeatures.toarray(), trainOutputsModel)"
   ],
   "id": "6f2ba0c2f6707988",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 0.7035\n",
      "Iteration 10, loss = 0.7025\n",
      "Iteration 20, loss = 0.7016\n",
      "Iteration 30, loss = 0.7006\n",
      "Iteration 40, loss = 0.6997\n",
      "Iteration 50, loss = 0.6988\n",
      "Iteration 60, loss = 0.6978\n",
      "Iteration 70, loss = 0.6969\n",
      "Iteration 80, loss = 0.6960\n",
      "Iteration 90, loss = 0.6951\n",
      "Iteration 100, loss = 0.6943\n",
      "Iteration 110, loss = 0.6934\n",
      "Iteration 120, loss = 0.6925\n",
      "Iteration 130, loss = 0.6916\n",
      "Iteration 140, loss = 0.6908\n",
      "Iteration 150, loss = 0.6899\n",
      "Iteration 160, loss = 0.6890\n",
      "Iteration 170, loss = 0.6882\n",
      "Iteration 180, loss = 0.6874\n",
      "Iteration 190, loss = 0.6865\n",
      "Iteration 200, loss = 0.6857\n",
      "Iteration 210, loss = 0.6849\n",
      "Iteration 220, loss = 0.6841\n",
      "Iteration 230, loss = 0.6833\n",
      "Iteration 240, loss = 0.6825\n",
      "Iteration 250, loss = 0.6817\n",
      "Iteration 260, loss = 0.6809\n",
      "Iteration 270, loss = 0.6801\n",
      "Iteration 280, loss = 0.6793\n",
      "Iteration 290, loss = 0.6786\n",
      "Iteration 300, loss = 0.6778\n",
      "Iteration 310, loss = 0.6771\n",
      "Iteration 320, loss = 0.6763\n",
      "Iteration 330, loss = 0.6756\n",
      "Iteration 340, loss = 0.6748\n",
      "Iteration 350, loss = 0.6741\n",
      "Iteration 360, loss = 0.6733\n",
      "Iteration 370, loss = 0.6726\n",
      "Iteration 380, loss = 0.6719\n",
      "Iteration 390, loss = 0.6712\n"
     ]
    }
   ],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T16:24:40.790592Z",
     "start_time": "2025-04-28T16:24:40.779151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Step 5: Testare model\n",
    "computedTestOutputs = supervisedClassifier.predict(testFeatures)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"acc: \", accuracy_score(testOutputs, computedTestOutputs))"
   ],
   "id": "bea0cd061655851e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.7\n"
     ]
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T16:24:41.932321Z",
     "start_time": "2025-04-28T16:24:41.918600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Step 5: Testare model Manual\n",
    "computedTestOutputsManual = model.predict(testFeatures.toarray())\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"acc: \", accuracy_score(testOutputsModel, computedTestOutputsManual))"
   ],
   "id": "8cfd6bcd25be760f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.55\n"
     ]
    }
   ],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T16:21:29.731943Z",
     "start_time": "2025-04-28T16:21:29.669472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Step 6: Predict sentiment for a text\n",
    "\n",
    "input_text = [\"By choosing a bike over a car, I’m reducing my environmental footprint. Cycling promotes eco-friendly transportation, and I’m proud to be part of that movement.\"]\n",
    "\n",
    "input_features = vectorizer.transform(input_text)\n",
    "\n",
    "predicted_sentiment = supervisedClassifier.predict(input_features)\n",
    "predicted_sentimentManual = model.predict(input_features.toarray())\n",
    "\n",
    "\n",
    "print(predicted_sentiment)\n",
    "print(predicted_sentimentManual)\n"
   ],
   "id": "a0b93284a0241340",
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "The TF-IDF vectorizer is not fitted",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotFittedError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[125], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#Step 6: Predict sentiment for a text\u001B[39;00m\n\u001B[0;32m      3\u001B[0m input_text \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBy choosing a bike over a car, I’m reducing my environmental footprint. Cycling promotes eco-friendly transportation, and I’m proud to be part of that movement.\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m----> 5\u001B[0m input_features \u001B[38;5;241m=\u001B[39m \u001B[43mvectorizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_text\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m predicted_sentiment \u001B[38;5;241m=\u001B[39m supervisedClassifier\u001B[38;5;241m.\u001B[39mpredict(input_features)\n\u001B[0;32m      8\u001B[0m predicted_sentimentManual \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(input_features\u001B[38;5;241m.\u001B[39mtoarray())\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001B[0m, in \u001B[0;36mTfidfVectorizer.transform\u001B[1;34m(self, raw_documents)\u001B[0m\n\u001B[0;32m   2110\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform\u001B[39m(\u001B[38;5;28mself\u001B[39m, raw_documents):\n\u001B[0;32m   2111\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001B[39;00m\n\u001B[0;32m   2112\u001B[0m \n\u001B[0;32m   2113\u001B[0m \u001B[38;5;124;03m    Uses the vocabulary and document frequencies (df) learned by fit (or\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2124\u001B[0m \u001B[38;5;124;03m        Tf-idf-weighted document-term matrix.\u001B[39;00m\n\u001B[0;32m   2125\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2126\u001B[0m     \u001B[43mcheck_is_fitted\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mThe TF-IDF vectorizer is not fitted\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2128\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mtransform(raw_documents)\n\u001B[0;32m   2129\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf\u001B[38;5;241m.\u001B[39mtransform(X, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1757\u001B[0m, in \u001B[0;36mcheck_is_fitted\u001B[1;34m(estimator, attributes, msg, all_or_any)\u001B[0m\n\u001B[0;32m   1754\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m   1756\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001B[1;32m-> 1757\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NotFittedError(msg \u001B[38;5;241m%\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(estimator)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m})\n",
      "\u001B[1;31mNotFittedError\u001B[0m: The TF-IDF vectorizer is not fitted"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T16:24:44.382178Z",
     "start_time": "2025-04-28T16:24:44.368167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Step 6: Predict sentiment for a text using Doc2Vec\n",
    "\n",
    "input_text = [\"By choosing a bike over a car, I’m reducing my environmental footprint. Cycling promotes eco-friendly transportation, and I’m proud to be part of that movement.\"]\n",
    "input_features = np.array([doc2vec_model.infer_vector(text.split()) for text in input_text])\n",
    "predicted_sentiment = supervisedClassifier.predict(input_features)\n",
    "predicted_sentimentManual = model.predict(input_features)\n",
    "\n",
    "\n",
    "print(predicted_sentiment)\n",
    "print(predicted_sentimentManual)\n"
   ],
   "id": "2ad5d3ca4ae89197",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative']\n",
      "[[0]]\n"
     ]
    }
   ],
   "execution_count": 138
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
